In this chapter, we will look at another implementation of maps called
\todef{Log-Structured Merge Trees (LSMs)}.\footnote{For once in this class,
  the word ``log'' does not refer to a logarithm, but to a list of
  items written down in order.}
They were introduced in Google's now-famous ``Big Table'' paper,
laying out an architecture for massive maps that are optimized for
sequential interaction with hard disks. They are now implemented in a
number of real-world database systems. The exact architecture
motivations for this data structure are beyond what we can cover in
this class, but you are welcome to search for more information online
and read up on why certain design choices are made.

For our purposes, they serve the twin purposes of (1) showing you a
neat, easy-to-implement, and perhaps unexpected implementation of maps
with somewhat different running times for the operations, and (2)
giving a good illustration of amortized analysis.
The simple LSM implementation we consider here will implement \code{add}
in amortized $O(\log n)$ per operation, and \code{get} in amortized
$O((\log n)^2)$ per operation. (We will not implement \code{remove}, as
that would break our nice amortized analysis.)
Notice the somewhat counterintuitive fact that insertion is
\emph{faster} than lookups.

In our simple LSMs, we have a linked list of sorted arrays of sizes
$1, 2, 4, 8, \ldots$.
To be more precise, for each $k$, we will either have one sorted array
of size exactly $2^k$, or no sorted array of that size (and, say, a
\code{NULL} pointer instead). 
These arrays are chained together in a linked list. 
Each array is sorted individually, but there is no sorting across
arrays. Figure~\ref{fig:lists-of-arrays-example} shows an example of
what you might get.

\begin{figure}[htb]
\begin{center}
\psset{unit=1cm,arrowsize=0.1 3,framesep=0.15}
\pspicture(-1.5,-5)(13,0.5)

\cnodeput(0,0){b0}{\phantom{5}}
\cnodeput(3,0){b1}{\phantom{5}}
\cnodeput(6,0){b2}{\phantom{5}}
\cnodeput(9,0){b3}{\phantom{5}}

\cnodeput[fillstyle=solid,fillcolor=black](12,0){b4}{}
\rput(-2,0){\pnode{h}}

\ncline{->}{h}{b0} \naput{head}
\ncline{->}{b0}{b1}
\ncline{->}{b1}{b2}
\ncline{->}{b2}{b3}
\ncline{->}{b3}{b4}

\rput(0,-1){\rnode{a0}{\multirput*{0}(-0.25,-0.5)(0,-0.5){1}{\psframe(0,0)(0.5,0.5)}
\rput(0,-0.25){\small 5}
}}

\rput(3,-1){\rnode{a1}{\multirput*{0}(-0.25,-0.5)(0,-0.5){2}{\psframe(0,0)(0.5,0.5)}
\rput(0,-0.25){\small 2}
\rput(0,-0.75){\small 4}
}}

\cnodeput[fillstyle=solid,fillcolor=black](6,-1){a2}{}

\rput(9,-1){\rnode{a3}{\multirput*{0}(-0.25,-0.5)(0,-0.5){8}{\psframe(0,0)(0.5,0.5)}
\rput(0,-0.25){\small 0}
\rput(0,-0.75){\small 1}
\rput(0,-1.25){\small 6}
\rput(0,-1.75){\small 9}
\rput(0,-2.25){\small 12}
\rput(0,-2.75){\small 14}
\rput(0,-3.25){\small 18}
\rput(0,-3.75){\small 20}
}}

\ncline[doubleline=true,arrowsize=0.1 1]{->}{b0}{a0}
\ncline[doubleline=true,arrowsize=0.1 1]{->}{b1}{a1}
\ncline[doubleline=true,arrowsize=0.1 1]{->}{b2}{a2}
\ncline[doubleline=true,arrowsize=0.1 1]{->}{b3}{a3}

\endpspicture
\end{center}
\caption{An LSM with 11 elements. Little black circles
  denote \code{NULL} pointers. Double lines denote pointers to
  arrays, while single lines are the overall linked list of arrays.
  We currently have arrays of sizes 1, 2, 8, but no array of size 4.
\label{fig:lists-of-arrays-example}}
\end{figure}

Because each array is sorted, we can perform Binary Search in
it, which takes at most $\Theta(\log (2^k)) = \Theta(k)$ in the
\Kth{k} array. 
Since we don't know which array to search in for a given key, in the
worst case, we have to try each of the arrays (for $k$ ranging from
$0$ to $\log_2 n$), giving us a time of
$\sum_{k=0}^{\log n} \Theta(k) = \Theta((\log n)^2)$ for the
\code{get} operation.

To add a new key, we create a new array of size 1 for it, and try to
insert it as the first array ($k=0$). Of course, this does not work if
there is already an array of size 1. In that case, we merge those two
arrays into a sorted array of size 2, and try to insert that at $k=1$.
If there is already an array of size 2, we again merge the two arrays
of size 2 each into a new array of size 4, and try to insert that. 
And so on. In each step, if the \Kth{k} position is already taken, 
we merge the two current arrays of size $2^k$ (using the \code{merge}
function (Algorithm~\ref{alg:sorting:merge}) from MergeSort --- see
Chapter~\ref{chap:sorting}) into an array of size $2^{k+1}$ and
continue with that one, replacing the array of size $2^k$ with \code{NULL}. 
For example, if we insert the key 3 into the structure of
Figure~\ref{fig:lists-of-arrays-example}, we will first produce the
array $[3]$, and merge it with $[5]$, to produce $[3,5]$. This is
merged with $[2,4]$, to produce $[2,3,4,5]$, which is then put in
position $k=2$. The final state will look as shown in
Figure~\ref{fig:lists-of-arrays-after}: 

\begin{figure}[htb]
\begin{center}
\psset{unit=1cm,arrowsize=0.1 3,framesep=0.15}
\pspicture(-1.5,-5)(13,0.5)

\cnodeput(0,0){b0}{\phantom{5}}
\cnodeput(3,0){b1}{\phantom{5}}
\cnodeput(6,0){b2}{\phantom{5}}
\cnodeput(9,0){b3}{\phantom{5}}

\cnodeput[fillstyle=solid,fillcolor=black](12,0){b4}{}
\rput(-2,0){\pnode{h}}

\ncline{->}{h}{b0} \naput{head}
\ncline{->}{b0}{b1}
\ncline{->}{b1}{b2}
\ncline{->}{b2}{b3}
\ncline{->}{b3}{b4}

\cnodeput[fillstyle=solid,fillcolor=black](0,-1){a0}{}

\cnodeput[fillstyle=solid,fillcolor=black](3,-1){a1}{}

\rput(6,-1){\rnode{a2}{\multirput*{0}(-0.25,-0.5)(0,-0.5){4}{\psframe(0,0)(0.5,0.5)}
\rput(0,-0.25){\small 2}
\rput(0,-0.75){\small 3}
\rput(0,-1.25){\small 4}
\rput(0,-1.75){\small 5}
}}

\rput(9,-1){\rnode{a3}{\multirput*{0}(-0.25,-0.5)(0,-0.5){8}{\psframe(0,0)(0.5,0.5)}
\rput(0,-0.25){\small 0}
\rput(0,-0.75){\small 1}
\rput(0,-1.25){\small 6}
\rput(0,-1.75){\small 9}
\rput(0,-2.25){\small 12}
\rput(0,-2.75){\small 14}
\rput(0,-3.25){\small 18}
\rput(0,-3.75){\small 20}
}}

\ncline[doubleline=true,arrowsize=0.1 1]{->}{b0}{a0}
\ncline[doubleline=true,arrowsize=0.1 1]{->}{b1}{a1}
\ncline[doubleline=true,arrowsize=0.1 1]{->}{b2}{a2}
\ncline[doubleline=true,arrowsize=0.1 1]{->}{b3}{a3}

\endpspicture
\end{center}
\caption{The list of arrays after inserting the key 3.
\label{fig:lists-of-arrays-after}}
\end{figure}

\section{Amortized Analysis of Insertion}

First, notice that in the worst case (when all the positions in the
linked list are already taken by arrays), we will be performing a
sequence of merges ending with an array of size $n$. 
We recall from our analysis of MergeSort that the \code{merge}
function takes linear time. Thus, the time to get an array of size $n$
was $\Theta(1+2+4+\ldots+n) = \Theta(n)$.
Thus, the worst-case time of the \code{add} function is $\Theta(n)$.
But we also see that this worst case should not happen often.
In fact, half the time, when we insert an element, the array in
position 0 is empty, so we can just insert the element.
Of the remaining cases, half the time (so a quarter total), the next
position (the array of size 2) is empty.
More generally, in a $2^{-k}$ fraction of the cases, the first $k-1$
arrays were taken, but the \Kth{k} array is available.
(In this way, this data structure resembles the binary counter from
Chapter~\ref{chap:amortized}.)

When the merging stops in the \Kth{k} array, we have an array of size
$2^k$, and have spent time $O(1)+O(2)+O(4)+\ldots+O(2^k) = O(2^k)$ to
produce it. (Notice that this is where the data structure differs from
the binary counter --- there, when we finish in digit $k$, we only
spent $k$ time total.) 
So the total time for a sequence of $n$ insertions (resulting in a
data structure with $n$ items in it) is
\[ \sum_{k=0}^{\log_2 n} \frac{n}{2^k} \cdot O(2^k)
\; = \; O(\sum_{k=0}^{\log_2 n} n) \; = \; O(n \log n). \]
Hence, the amortized cost per insertion is $O(\log n)$.

\smallskip

Now, to practice credit schemes, let's try that approach.
Here, the idea is that costs are incurred any time an array is merged. 
We will give each element its own little credit account, so
that it can pay for its share of the merge.
When does an element need to pay? 
Every time it moves from some array $k$ to the next one $k+1$.
If each element that is being merged can chip in one dollar then, we
get a linear number of dollars for a linear merge cost, so the merge
is all paid for. 
So when we first put a new element into the structure, we should give
it enough credit to pay for all its future moves.

But how many times will it move in the future? We don't know that at
the time of insertion, so how much credit do we give the element?
At this point, it is useful to remember that the whole ``credit'' idea
is only a part of the analysis; the implementation of the data
structure knows nothing about credits given to any elements.
And at the point we are analyzing a sequence of insertions, we know
how long the sequence is (say, $n$ insertions, for a final size of $n$), 
and hence, we know how many times the element will be moved ($\log n$
times). So we endow the element with $\log n$ credits initially.
Then, we maintain the invariant that each element sitting in the array
at level $k$ has $\log n - k$ credits left. Elements then use one of
their credits for each merge, which maintains the invariant.

So in summary, we have shown in a second way that insertions into the
list of arrays structure take $O(\log n)$ steps amortized in the worst
case.

\section{Some Remarks on Efficient Implementation}
Here are a few thoughts on the implementation in real ``big data''
systems, which also motivate the name ``Log-Structured Merge Trees''.
Each individual array is like a ``log'' of some sorted data.
Typically, the smallest array (which in practical implementations will
not have size 1, but a ``small'' size like $10^{8}$ that fits in main
memory easily) will be kept in main memory, and all the larger ones on
disk. In fact, the small array in main memory is not kept in an array,
but in an efficient data structure like a balanced search tree.

When the ``small array'' (or search tree) gets too big, it is written
out into its own sorted file on disk. Thus, over time, more and more
files accumulate. Every now and then, the system cleans up those files
by merging files of the same size repeatedly, just as we discussed.
This doesn't necessarily have to happen as soon as there are two files
of the same size; as you saw in the analysis, the \code{get} function
gets slower as there are more arrays/files, so we need to merge them
before the number of files becomes too large, but the exact time does
not matter too much.

The important thing is that the files on disk can be merged (and the
new file written) with linear scans through them, making access much
more effecient. On the other hand, searching in any one array/file
only takes logarithmically many steps, and thus is also still fairly
efficient.

\smallskip

At this point, let us also briefly revisit that funky $\log^2 n$ cost
for the \code{get} operation. It arose basically because we have 
$\Theta(\log n)$ arrays, and in each of them, we perform a binary search
costing essentially $\Theta(\log n)$. (In some of them, the cost is
less, but there are enough arrays for which the cost is $\Theta(\log n)$.)
If only there were a way to diagnose without binary search whether an
element is in the array; then, we could just skip the arrays known not
to contain the element, and binary search only in the right one.
Do we know of any data structure that answers the query 
``Is this element present?''

In fact, we do. Sets are designed exactly for this purpose. Of course,
since we are trying to build a map in the first place, assuming that
we have a set implementation feels a bit like cheating.
But notice that our set implementation does not have to be perfect.
It's o.k.~if it sometimes claims that an element is in an array when
it really isn't. The only result will be that we do an unnecessary
binary search, slowing us down a bit. But nothing breaks.
So an approximate set implementation will be just fine.
And we just saw how to do that: In Section~\ref{sec:bloom-filters}, we
learned about Bloom Filters, which are a lightweight use of hash
functions to build an approximate set implementation.
So we can use a Bloom Filter for each of the arrays/logs to save
ourselves 99\% of the binary searches, and drastically speed up the
\code{get} operation. And this is one of the main reasons why Bloom
Filters have seen so much interest recently.
